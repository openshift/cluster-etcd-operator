// Code generated by go-bindata.
// sources:
// bindata/etcd/cm.yaml
// bindata/etcd/defaultconfig.yaml
// bindata/etcd/etcd-member-remove.sh
// bindata/etcd/etcd-restore-backup.sh
// bindata/etcd/etcd-snapshot-backup.sh
// bindata/etcd/ns.yaml
// bindata/etcd/openshift-recovery-tools
// bindata/etcd/pod-cm.yaml
// bindata/etcd/pod.yaml
// bindata/etcd/restore-pod-cm.yaml
// bindata/etcd/restore-pod.yaml
// bindata/etcd/sa.yaml
// bindata/etcd/scripts-cm.yaml
// bindata/etcd/svc.yaml
// DO NOT EDIT!

package etcd_assets

import (
	"fmt"
	"io/ioutil"
	"os"
	"path/filepath"
	"strings"
	"time"
)

type asset struct {
	bytes []byte
	info  os.FileInfo
}

type bindataFileInfo struct {
	name    string
	size    int64
	mode    os.FileMode
	modTime time.Time
}

func (fi bindataFileInfo) Name() string {
	return fi.name
}
func (fi bindataFileInfo) Size() int64 {
	return fi.size
}
func (fi bindataFileInfo) Mode() os.FileMode {
	return fi.mode
}
func (fi bindataFileInfo) ModTime() time.Time {
	return fi.modTime
}
func (fi bindataFileInfo) IsDir() bool {
	return false
}
func (fi bindataFileInfo) Sys() interface{} {
	return nil
}

var _etcdCmYaml = []byte(`apiVersion: v1
kind: ConfigMap
metadata:
  namespace: openshift-etcd
  name: config
data:
  config.yaml:
`)

func etcdCmYamlBytes() ([]byte, error) {
	return _etcdCmYaml, nil
}

func etcdCmYaml() (*asset, error) {
	bytes, err := etcdCmYamlBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/cm.yaml", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdDefaultconfigYaml = []byte(`apiVersion: kubecontrolplane.config.openshift.io/v1
kind: EtcdConfig
`)

func etcdDefaultconfigYamlBytes() ([]byte, error) {
	return _etcdDefaultconfigYaml, nil
}

func etcdDefaultconfigYaml() (*asset, error) {
	bytes, err := etcdDefaultconfigYamlBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/defaultconfig.yaml", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdEtcdMemberRemoveSh = []byte(`#!/usr/bin/env bash

# example
# sudo ./etcd-member-remove.sh $etcd_name

if [[ $EUID -ne 0 ]]; then
  echo "This script must be run as root"
  exit 1
fi

usage () {
    echo 'The name of the etcd member to remove is required: ./etcd-member-remove.sh $etcd_name'
    exit 1
}

if [ "$1" == "" ]; then
    usage
fi

ETCD_NAME=$1
ASSET_DIR=/home/core/assets
ASSET_DIR_TMP="$ASSET_DIR/tmp"
ETCDCTL=$ASSET_DIR/bin/etcdctl
ETCD_DATA_DIR=/var/lib/etcd
CONFIG_FILE_DIR=/etc/kubernetes

source "/usr/local/bin/openshift-recovery-tools"

function run {
  init
  dl_etcdctl
  backup_etcd_client_certs
  etcd_member_remove $ETCD_NAME
}

run

`)

func etcdEtcdMemberRemoveShBytes() ([]byte, error) {
	return _etcdEtcdMemberRemoveSh, nil
}

func etcdEtcdMemberRemoveSh() (*asset, error) {
	bytes, err := etcdEtcdMemberRemoveShBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/etcd-member-remove.sh", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdEtcdRestoreBackupSh = []byte(`#!/usr/bin/env bash

set -o errexit
set -o pipefail

# example
# ./etcd-snapshot-restore.sh $path-to-backup 

if [[ $EUID -ne 0 ]]; then
  echo "This script must be run as root"
  exit 1
fi

usage () {
    echo 'Path to the directory containing backup files is required: ./etcd-snapshot-restore.sh <path-to-backup>'
    exit 1
}

if [ "$1" == "" ]; then
    usage
fi

ASSET_DIR=/home/core/assets

RESTORE_STATIC_RESOURCES="true"
if [ -f "$1" ]; then
  # For backward-compatibility, we support restoring from single snapshot.db file or single tar.gz file
  if [[ "$1" =~ \.db$ ]]; then
    RESTORE_STATIC_RESOURCES="false"
    SNAPSHOT_FILE="$1"
  elif [[ "$1" =~ \.tar\.gz$ ]]; then
    BACKUP_FILE="$1"
    tar xzf ${BACKUP_FILE} -C ${ASSET_DIR}/tmp/ snapshot.db
    SNAPSHOT_FILE="${ASSET_DIR}/tmp/snapshot.db"
  else
    usage
  fi
elif [ -d "$1" ]; then
  BACKUP_FILE=$(ls -vd "$1"/static_kuberesources*.tar.gz | tail -1) || true
  SNAPSHOT_FILE=$(ls -vd "$1"/snapshot*.db | tail -1) || true
  if [ ! -f ${BACKUP_FILE}  -o ! -f ${SNAPSHOT_FILE} ]; then
    usage
  fi
else
  usage
fi

CONFIG_FILE_DIR=/etc/kubernetes
MANIFEST_DIR="${CONFIG_FILE_DIR}/manifests"
MANIFEST_STOPPED_DIR="${ASSET_DIR}/manifests-stopped"
RUN_ENV=/run/etcd/environment

ETCDCTL="${ASSET_DIR}/bin/etcdctl"
ETCD_DATA_DIR=/var/lib/etcd
ETCD_DATA_DIR_BACKUP=/var/lib/etcd-backup
RESTORE_ETCD_POD_YAML="${CONFIG_FILE_DIR}/static-pod-resources/etcd-certs/configmaps/restore-etcd-pod/pod.yaml"
ETCD_MANIFEST="${MANIFEST_DIR}/etcd-pod.yaml"
ETCD_STATIC_RESOURCES="${CONFIG_FILE_DIR}/static-pod-resources/etcd-member"
STOPPED_STATIC_PODS="${ASSET_DIR}/tmp/stopped-static-pods"

source "/usr/local/bin/openshift-recovery-tools"

function run {
  init
  if [ ! -f "${SNAPSHOT_FILE}" ]; then
    echo "etcd snapshot ${SNAPSHOT_FILE} does not exist."
    exit 1
  fi

  dl_etcdctl
  backup_manifest
  stop_static_pods
  stop_etcd
  stop_kubelet
  stop_all_containers
  backup_data_dir
  remove_data_dir
  [ "${RESTORE_STATIC_RESOURCES}" = "true" ] && remove_kube_static_resources
  copy_snapshot_to_backupdir
  [ "${RESTORE_STATIC_RESOURCES}" = "true" ] && restore_kube_static_resources
  start_static_pods
  copy_etcd_restore_pod_yaml
  start_kubelet
}

run

`)

func etcdEtcdRestoreBackupShBytes() ([]byte, error) {
	return _etcdEtcdRestoreBackupSh, nil
}

func etcdEtcdRestoreBackupSh() (*asset, error) {
	bytes, err := etcdEtcdRestoreBackupShBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/etcd-restore-backup.sh", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdEtcdSnapshotBackupSh = []byte(`#!/usr/bin/env bash

set -o errexit
set -o pipefail
set -o errtrace

# example
# etcd-snapshot-backup.sh $path-to-snapshot

if [[ $EUID -ne 0 ]]; then
  echo "This script must be run as root"
  exit 1
fi

usage () {
    echo 'Path to backup dir required: ./etcd-snapshot-backup.sh <path-to-backup-dir>'
    exit 1
}

ASSET_DIR=/home/core/assets

if [ -z "$1" ] || [ -f "$1" ]; then
  usage
fi

if [ ! -d "$1" ]; then
  mkdir -p $1
fi

BACKUP_DIR="$1"
DATESTRING=$(date "+%F_%H%M%S")
BACKUP_TAR_FILE=${BACKUP_DIR}/static_kuberesources_${DATESTRING}.tar.gz
SNAPSHOT_FILE="${BACKUP_DIR}/snapshot_${DATESTRING}.db"

trap "rm -f ${BACKUP_TAR_FILE} ${SNAPSHOT_FILE}" ERR

CONFIG_FILE_DIR=/etc/kubernetes
MANIFEST_DIR="${CONFIG_FILE_DIR}/manifests"
MANIFEST_STOPPED_DIR="${ASSET_DIR}/manifests-stopped"
ETCDCTL="${ASSET_DIR}/bin/etcdctl"
ETCD_DATA_DIR=/var/lib/etcd
ETCD_MANIFEST="${MANIFEST_DIR}/etcd-pod.yaml"
ETCD_STATIC_RESOURCES="${CONFIG_FILE_DIR}/static-pod-resources/etcd-member"
STOPPED_STATIC_PODS="${ASSET_DIR}/tmp/stopped-static-pods"

source "/usr/local/bin/openshift-recovery-tools"

function run {
  init
  dl_etcdctl
  backup_etcd_client_certs
  backup_manifest
  backup_latest_kube_static_resources
  snapshot_data_dir
  echo "snapshot db and kube resources are successfully saved to ${BACKUP_DIR}!"
}

run

`)

func etcdEtcdSnapshotBackupShBytes() ([]byte, error) {
	return _etcdEtcdSnapshotBackupSh, nil
}

func etcdEtcdSnapshotBackupSh() (*asset, error) {
	bytes, err := etcdEtcdSnapshotBackupShBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/etcd-snapshot-backup.sh", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdNsYaml = []byte(`apiVersion: v1
kind: Namespace
metadata:
  annotations:
    openshift.io/node-selector: ""
  name: openshift-etcd
  labels:
    openshift.io/run-level: "0"
`)

func etcdNsYamlBytes() ([]byte, error) {
	return _etcdNsYaml, nil
}

func etcdNsYaml() (*asset, error) {
	bytes, err := etcdNsYamlBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/ns.yaml", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdOpenshiftRecoveryTools = []byte(`#!/usr/bin/env bash
export ETCDCTL_API=3

ETCDCTL_WITH_TLS="$ETCDCTL --cert $ASSET_DIR/backup/etcd-client.crt --key $ASSET_DIR/backup/etcd-client.key --cacert $ASSET_DIR/backup/etcd-ca-bundle.crt"

init() {
  ASSET_BIN=${ASSET_DIR}/bin
  if [ ! -d "$ASSET_BIN" ]; then
    echo "Creating asset directory ${ASSET_DIR}"
    for dir in {bin,tmp,shared,backup,templates,restore,manifests}; do
      /usr/bin/mkdir -p ${ASSET_DIR}/${dir}
    done
  fi
}

# download and test etcdctl from upstream release assets
dl_etcdctl() {
  ETCDCTL_BIN="${ASSET_DIR}/bin/etcdctl"
  local etcdimg="registry.svc.ci.openshift.org/ci-op-l7y8kffx/stable@sha256:c576980bdce812e5d000e6f96fc238d75688da01f767cdaed6cbf04c40c600fa"
  local etcdctr=$(podman create ${etcdimg})
  local etcdmnt=$(podman mount "${etcdctr}")
  cp ${etcdmnt}/bin/etcdctl $ASSET_DIR/bin
  umount "${etcdmnt}"
  podman rm "${etcdctr}"
  $ETCDCTL_BIN version
}

#backup etcd client certs
backup_etcd_client_certs() {
  echo "Trying to backup etcd client certs.."
  if [ -f "$ASSET_DIR/backup/etcd-ca-bundle.crt" ] && [ -f "$ASSET_DIR/backup/etcd-client.crt" ] && [ -f "$ASSET_DIR/backup/etcd-client.key" ]; then
     echo "etcd client certs already backed up and available $ASSET_DIR/backup/"
  else
    STATIC_DIRS=($(ls -td "${CONFIG_FILE_DIR}"/static-pod-resources/kube-apiserver-pod-[0-9]*)) || true
    if [ -z "${STATIC_DIRS}" ]; then
      echo "error finding static-pod-resources"
      exit 1
    fi
    for APISERVER_POD_DIR in "${STATIC_DIRS[@]}"; do
      SECRET_DIR="${APISERVER_POD_DIR}/secrets/etcd-client"
      CONFIGMAP_DIR="${APISERVER_POD_DIR}/configmaps/etcd-serving-ca"
      if [ -f "$CONFIGMAP_DIR/ca-bundle.crt" ] && [ -f "$SECRET_DIR/tls.crt" ] && [ -f "$SECRET_DIR/tls.key" ]; then
        echo "etcd client certs found in $APISERVER_POD_DIR backing up to $ASSET_DIR/backup/"
        cp $CONFIGMAP_DIR/ca-bundle.crt $ASSET_DIR/backup/etcd-ca-bundle.crt
        cp $SECRET_DIR/tls.crt $ASSET_DIR/backup/etcd-client.crt
        cp $SECRET_DIR/tls.key $ASSET_DIR/backup/etcd-client.key
        return 0
      else
        echo "${APISERVER_POD_DIR} does not contain etcd client certs, trying next .."
      fi
    done
    echo "backup failed: client certs not found"
    exit 1
  fi
}

#backup latest static pod resources for kube-apiserver
backup_latest_kube_static_resources() {
  echo "Trying to backup latest static pod resources.."
  LATEST_STATIC_POD_DIR=$(ls -vd "${CONFIG_FILE_DIR}"/static-pod-resources/kube-apiserver-pod-[0-9]* | tail -1) || true
  if [ -z "$LATEST_STATIC_POD_DIR" ]; then
      echo "error finding static-pod-resources"
      exit 1
  fi

  LATEST_ETCD_STATIC_POD_DIR=$(ls -vd "${CONFIG_FILE_DIR}"/static-pod-resources/etcd-pod-[0-9]* | tail -1) || true
  if [ -z "$LATEST_ETCD_STATIC_POD_DIR" ]; then
      echo "error finding static-pod-resources"
      exit 1
  fi

  # tar up the static kube resources, with the path relative to CONFIG_FILE_DIR
  tar -cpzf $BACKUP_TAR_FILE -C ${CONFIG_FILE_DIR} ${LATEST_STATIC_POD_DIR#$CONFIG_FILE_DIR/} ${LATEST_ETCD_STATIC_POD_DIR#$CONFIG_FILE_DIR/}
}

append_snapshot_to_tar_and_gzip() {
  # "r" flag is used to append snapshot.db to the existing tar archive
  tar rf ${BACKUP_TAR_FILE} -C ${ASSET_DIR}/tmp snapshot.db
  gzip ${BACKUP_TAR_FILE}
}

# backup current etcd-member pod manifest
backup_manifest() {
  if [ -e "${ASSET_DIR}/backup/etcd-pod.yaml" ]; then
    echo "etcd-pod.yaml found in ${ASSET_DIR}/backup/"
  else
    echo "Backing up ${ETCD_MANIFEST} to ${ASSET_DIR}/backup/"
    cp ${ETCD_MANIFEST} ${ASSET_DIR}/backup/
  fi
}

# backup etcd.conf
backup_etcd_conf() {
  if [ -e "${ASSET_DIR}/backup/etcd.conf" ]; then
    echo "etcd.conf backup upready exists $ASSET_DIR/backup/etcd.conf"
  else
    echo "Backing up /etc/etcd/etcd.conf to ${ASSET_DIR}/backup/"
    cp /etc/etcd/etcd.conf ${ASSET_DIR}/backup/
  fi
}

backup_data_dir() {
  if [ -f "$ASSET_DIR/backup/etcd/member/snap/db" ]; then
    echo "etcd data-dir backup found $ASSET_DIR/backup/etcd.."
  elif [ ! -f "${ETCD_DATA_DIR}/member/snap/db" ]; then
    echo "Local etcd snapshot file not found, backup skipped.."
  else
    echo "Backing up etcd data-dir.."
    cp -rap ${ETCD_DATA_DIR} $ASSET_DIR/backup/
  fi
}

snapshot_data_dir() {
  ${ETCDCTL_WITH_TLS} snapshot save ${SNAPSHOT_FILE}
}

# backup etcd peer, server and metric certs
backup_certs() {
  COUNT=$(ls $ETCD_STATIC_RESOURCES/system\:etcd-* 2>/dev/null | wc -l) || true
  BACKUP_COUNT=$(ls $ASSET_DIR/backup/system\:etcd-* 2>/dev/null | wc -l) || true

  if [ "$BACKUP_COUNT" -gt 1 ]; then
    echo "etcd TLS certificate backups found in $ASSET_DIR/backup.."
  elif [ "$COUNT" -eq 0 ]; then
    echo "etcd TLS certificates not found, backup skipped.."
  else
    echo "Backing up etcd certificates.."
    cp $ETCD_STATIC_RESOURCES/system\:etcd-* $ASSET_DIR/backup/
  fi
}

# stop etcd by moving the manifest out of /etcd/kubernetes/manifests
# we wait for all etcd containers to die.
stop_etcd() {
  echo "Stopping etcd.."

  if [ ! -d "$MANIFEST_STOPPED_DIR" ]; then
    mkdir $MANIFEST_STOPPED_DIR
  fi

  if [ -e "$ETCD_MANIFEST" ]; then
    mv $ETCD_MANIFEST $MANIFEST_STOPPED_DIR
  fi

  for name in {etcd,etcd-metric}
  do
    while false && [ ! -z "$(crictl pods -name $name --state Ready -q)" ]; do
      echo "Waiting for $name to stop"
      sleep 10
    done
  done
}

remove_data_dir() {
  echo "Removing etcd data-dir ${ETCD_DATA_DIR}"
  rm -rf ${ETCD_DATA_DIR}
}

remove_certs() {
  COUNT=$(ls $ETCD_STATIC_RESOURCES/system\:etcd-* 2>/dev/null | wc -l) || true
  if [ "$COUNT" -gt 1 ]; then
     echo "Removing etcd certs.."
     rm -f $ETCD_STATIC_RESOURCES/system\:etcd-*
  else
     echo "remove_certs: etcd TLS certificates are not found."
  fi
}

remove_kube_static_resources() {
  # Only remove those directories that are greater or equal to the backed up revision.
  REVISION=$(tar tf $BACKUP_FILE | grep -oP "(?<=static-pod-resources/kube-apiserver-pod-)[0-9]*" | head -1) || true
  if [ ! -z "${REVISION}" ]; then
     KUBE_DIRS=$(ls -vd ${CONFIG_FILE_DIR}/static-pod-resources/kube-apiserver-pod-[0-9]* | awk -F"-" -v REV="${REVISION}" '{ if ($NF >= REV) print }') || true
     if [ ! -z "${KUBE_DIRS}" ]; then
        echo "Removing newer static pod resources..."
        rm -rf ${KUBE_DIRS}
     else
        echo "remove_kube_static_resources: newer revisions of kube-apiserver-pod static resources are not found."
     fi
  fi

  # remove newer etcd static pod resources only if the backup contains etcd-pod resources
  ETCD_REVISION=$(tar tf $BACKUP_FILE | grep -oP "(?<=static-pod-resources/etcd-pod-)[0-9]*" | head -1) || true
  if [ ! -z "${ETCD_REVISION}" ]; then
     ETCD_DIRS=$(ls -vd ${CONFIG_FILE_DIR}/static-pod-resources/etcd-pod-[0-9]* | awk -F"-" -v REV="${ETCD_REVISION}" '{ if ($NF >= REV) print }') || true
     if [ ! -z "${ETCD_DIRS}" ]; then
        echo "Removing newer etcd pod resources..."
        rm -rf ${ETCD_DIRS}
     else
        echo "remove_kube_static_resources: newer revisions of etcd-pod static resources are not found."
     fi
  fi
}

copy_etcd_restore_pod_yaml() {
  if [ ! -f "${RESTORE_ETCD_POD_YAML}" ]; then
    echo "File not found, restore failed: ${RESTORE_ETCD_POD_YAML}."
    exit 1
  fi
  echo "Copying ${RESTORE_ETCD_POD} to ${MANIFEST_DIR}"
  cp -p $RESTORE_ETCD_POD_YAML ${MANIFEST_DIR}/etcd-pod.yaml
}

copy_snapshot_to_backupdir() {
  if [ ! -f "$SNAPSHOT_FILE" ]; then
    echo "Snapshot file not found, restore failed: $SNAPSHOT_FILE."
    exit 1
  fi
  echo "Copying $SNAPSHOT_FILE to ${ETCD_DATA_DIR_BACKUP}"
  [ ! -d ${ETCD_DATA_DIR_BACKUP} ] && mkdir -p ${ETCD_DATA_DIR_BACKUP}
  cp -p $SNAPSHOT_FILE ${ETCD_DATA_DIR_BACKUP}/snapshot.db
}

restore_snapshot() {
  if [ ! -f "$SNAPSHOT_FILE" ]; then
    echo "Snapshot file not found, restore failed: $SNAPSHOT_FILE."
    exit 1
  fi

  echo "Restoring etcd member $ETCD_NAME from snapshot.."
  ${ETCDCTL} snapshot restore $SNAPSHOT_FILE \
    --name $ETCD_NAME \
    --initial-cluster ${ETCD_INITIAL_CLUSTER} \
    --initial-cluster-token etcd-cluster-1 \
    --skip-hash-check=true \
    --initial-advertise-peer-urls https://${ETCD_IPV4_ADDRESS}:2380 \
    --data-dir $ETCD_DATA_DIR
}

restore_kube_static_resources() {
  tar -C ${CONFIG_FILE_DIR} -xzf $BACKUP_FILE static-pod-resources
}

patch_manifest() {
  echo "Patching etcd-member manifest.."
  cp $ASSET_DIR/backup/etcd-pod.yaml $ASSET_DIR/tmp/etcd-pod.yaml.template
  sed -i /' '--discovery-srv/d $ASSET_DIR/tmp/etcd-pod.yaml.template
  mv $ASSET_DIR/tmp/etcd-pod.yaml.template $MANIFEST_STOPPED_DIR/etcd-pod.yaml
}

# generate a kubeconf like file for the cert agent to consume and contact signer.
gen_config() {
  CA=$(base64 $ASSET_DIR/backup/etcd-ca-bundle.crt | tr -d '\n') || true
  CERT=$(base64 $ASSET_DIR/backup/etcd-client.crt | tr -d '\n') || true
  KEY=$(base64 $ASSET_DIR/backup/etcd-client.key | tr -d '\n') || true

  cat > $ETCD_STATIC_RESOURCES/.recoveryconfig << EOF
clusters:
- cluster:
    certificate-authority-data: ${CA}
    server: https://${RECOVERY_SERVER_IP}:9943
  name: ${CLUSTER_NAME}
contexts:
- context:
    cluster: ${CLUSTER_NAME}
    user: kubelet
  name: kubelet
current-context: kubelet
preferences: {}
users:
- name: kubelet
  user:
    client-certificate-data: ${CERT}
    client-key-data: ${KEY}
EOF
}

# add member to cluster
etcd_member_add() {
  echo "Updating etcd membership.."
  if [ -d "$ETCD_DATA_DIR" ]; then
    echo "Removing etcd data_dir $ETCD_DATA_DIR.."
    rm -rf $ETCD_DATA_DIR
  fi

  RESPONSE=$($ETCDCTL_WITH_TLS --endpoints ${RECOVERY_SERVER_IP}:2379 member add $ETCD_NAME --peer-urls=https://${ETCD_DNS_NAME}:2380)
  if [ $? -eq 0 ]; then
    echo "$RESPONSE"
    APPEND_CONF=$(echo "$RESPONSE" | sed -e '1,2d')
    echo -e "\n\n#[recover]\n$APPEND_CONF" >> $ETCD_CONFIG
  else
    echo "$RESPONSE"
    exit 1
  fi
}

start_etcd() {
  echo "Starting etcd.."
  mv ${MANIFEST_STOPPED_DIR}/etcd-pod.yaml $MANIFEST_DIR
}

# remove member from cluster by name
etcd_member_remove() {
  NAME="$1"

  if [ -z "$NAME" ]; then
    echo "etcd_member_remove requires 1 argument NAME"
    exit 1
  fi

  ID=$($ETCDCTL_WITH_TLS member list | awk -F "," "/\s${NAME}\,/"'{print $1}') || true
  if [ "$?" -ne 0 ] || [ -z "$ID" ]; then
    echo "could not find etcd member $NAME to remove."
    exit 1
  fi

  $ETCDCTL_WITH_TLS member remove $ID
  if [ "$?" -ne 0 ]; then
    echo "removing etcd member $NAME with ID: $ID failed"
    exit 1
  fi
  echo "etcd member $NAME with $ID successfully removed.."
}

populate_template() {
  FIND="$1"
  REPLACE="$2"
  TEMPLATE="$3"
  OUT="$4"

  echo "Populating template $TEMPLATE"

  if [ -z "$FIND" ] || [ -z "$REPLACE" ] || [ -z "$TEMPLATE" ] || [ -z "$OUT" ]; then
    echo "populate_template requires 4 arguments FIND, REPLACE, TEMPLATE and OUT"
    exit 1
  elif [ ! -f "$TEMPLATE" ]; then
    echo "template $TEMPLATE does not exist"
    exit 1
  fi

  TMP_FILE=$(date +"%m-%d-%Y-%H%M")
  cp $TEMPLATE "$ASSET_DIR/tmp/${TMP_FILE}"

  sed -i "s|${FIND}|${REPLACE}|" "$ASSET_DIR/tmp/${TMP_FILE}"
  mv "$ASSET_DIR/tmp/${TMP_FILE}" "$OUT"
}

start_cert_recover() {
  echo "Starting etcd client cert recovery agent.."
  mv ${MANIFEST_STOPPED_DIR}/etcd-generate-certs.yaml $MANIFEST_DIR
}

verify_certs() {
  iterations=0
  while [ "$(ls $ETCD_STATIC_RESOURCES | wc -l)" -lt 9  ]; do
    let iterations=$iterations+1
    if [ $iterations -gt 60 ]; then
      echo "Failed to verify cert generation after 60 iterations. Exiting!"
      exit 1
    fi
    echo "Waiting for certs to generate... ($iterations/60)"
    sleep 10
  done
}

stop_cert_recover() {
  echo "Stopping cert recover.."

  if [ -f "${CONFIG_FILE_DIR}/manifests/etcd-generate-certs.yaml" ]; then
    mv ${CONFIG_FILE_DIR}/manifests/etcd-generate-certs.yaml $MANIFEST_STOPPED_DIR
  fi

  for name in {generate-env,generate-certs}; do
    while [ ! -z "$(crictl pods -name $name --state Ready -q)" ]; do
      echo "Waiting for $name to stop"
      sleep 10
    done
  done
}

stop_static_pods() {
  echo "Stopping all static pods.."

  if [ ! -d "$MANIFEST_STOPPED_DIR" ]; then
    mkdir $MANIFEST_STOPPED_DIR
  fi

  find ${MANIFEST_DIR} -maxdepth 1 -type f -printf "%f\n" > $STOPPED_STATIC_PODS

  while read STATIC_POD; do
    echo "..stopping $STATIC_POD"
    mv ${MANIFEST_DIR}/${STATIC_POD} $MANIFEST_STOPPED_DIR
  done <$STOPPED_STATIC_PODS
}

start_static_pods() {
  echo "Starting static pods.."
  find ${MANIFEST_STOPPED_DIR} -maxdepth 1 -type f -printf "%f\n" > $STOPPED_STATIC_PODS
  while read STATIC_POD; do
    echo "..starting $STATIC_POD"
    mv ${MANIFEST_STOPPED_DIR}/${STATIC_POD} $MANIFEST_DIR
  done <$STOPPED_STATIC_PODS
}

stop_kubelet() {
  echo "Stopping kubelet.."
  systemctl stop kubelet.service
}

start_kubelet() {
  echo "Starting kubelet.."
  systemctl daemon-reload
  systemctl start kubelet.service
}

stop_all_containers() {
  conids=$(crictl ps -q)
  iterations=0
  while [ ! -z "$conids" ]; do
      let iterations=$iterations+1
      if [ $iterations -ge 60 ]; then
          echo "Failed to stop all containers after 60 iterations. Exiting!"
          exit 1
      fi
      crictl stop $conids || true
      echo "Waiting for all containers to stop... ($iterations/60)"
      sleep 5
      conids=$(crictl ps -q)
  done
  echo "All containers are stopped."
}

# validate_environment performs the same actions as the discovery container in etcd-member init
# sometimes $RUN_ENV is not available if the node is rebooted so we recreate here.
validate_environment() {
  if [ -f "$RUN_ENV" ] && [ -s "$RUN_ENV" ];then
    return 0
  fi
  SRV_A_RECORD=$(dig +noall +answer SRV _etcd-server-ssl._tcp.${DISCOVERY_DOMAIN} | grep -oP '(?<=2380 ).*[^\.]' | xargs) || true
  HOST_IPS=$(ip -o addr |  grep -oP '(?<=inet )(\d{1,3}\.?){4}') || true

  if [ -z "$SRV_A_RECORD" ]; then
    echo "SRV A record query for ${DISCOVERY_DOMAIN} failed please update DNS"
    exit 1
  elif [ -z "$HOST_IPS" ]; then
    echo "Unable to find any IPv4 addresses for host interfaces"
    exit 1
  fi

  for a in ${SRV_A_RECORD[@]}; do
    echo "checking against $a"
    for i in ${HOST_IPS[@]}; do
      DIG_IP=$(dig +short $a)
      if [ -z "$DIG_IP" ]; then
        echo "No matching A record found for $a skipping"
        continue
      elif [ "$DIG_IP" == "$i" ]; then
        echo "dns name is $a"
        [ ! -d $(dirname "${RUN_ENV}") ] && mkdir -p $(dirname "${RUN_ENV}")
        cat > $RUN_ENV << EOF
ETCD_IPV4_ADDRESS=$DIG_IP
ETCD_DNS_NAME=$a
ETCD_WILDCARD_DNS_NAME=*.${DISCOVERY_DOMAIN}
EOF
        return 0
      fi
    done
  done
  echo "SRV query failed no matching records found"
  exit 1
}

# validate_etcd_name uses regex to return the etcd member name key from ETCD_INITIAL_CLUSTER matching the local ETCD_DNS_NAME.
validate_etcd_name() {
  ETCD_NAME=$(echo ${ETCD_INITIAL_CLUSTER} | grep -oP "(?<=)[^,,\s]*(?==[^=]*${ETCD_DNS_NAME}\b)") || true
  if [ -z "$ETCD_NAME" ]; then
    echo "Validating INITIAL_CLUSTER failed: ${ETCD_DNS_NAME} is not found in ${ETCD_INITIAL_CLUSTER}" >&2
    exit 1
  fi
  echo "$ETCD_NAME"
}

`)

func etcdOpenshiftRecoveryToolsBytes() ([]byte, error) {
	return _etcdOpenshiftRecoveryTools, nil
}

func etcdOpenshiftRecoveryTools() (*asset, error) {
	bytes, err := etcdOpenshiftRecoveryToolsBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/openshift-recovery-tools", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdPodCmYaml = []byte(`apiVersion: v1
kind: ConfigMap
metadata:
  namespace: openshift-etcd
  name: etcd-pod
data:
  pod.yaml:
  forceRedeploymentReason:
  version:
`)

func etcdPodCmYamlBytes() ([]byte, error) {
	return _etcdPodCmYaml, nil
}

func etcdPodCmYaml() (*asset, error) {
	bytes, err := etcdPodCmYamlBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/pod-cm.yaml", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdPodYaml = []byte(`apiVersion: v1
kind: Pod
metadata:
  name: etcd
  namespace: openshift-etcd
  labels:
    app: etcd
    k8s-app: etcd
    etcd: "true"
    revision: "REVISION"
spec:
  initContainers:
    - name: etc-quorum-guard-copy
      image: ${IMAGE}
      imagePullPolicy: IfNotPresent
      terminationMessagePolicy: FallbackToLogsOnError
      command:
        - /bin/sh
        - -c
        - |
          #!/bin/sh
          set -euo pipefail

          cp /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt /etc/kubernetes/etcd-backup-dir/system:etcd-peer-NODE_NAME.crt
          cp /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key /etc/kubernetes/etcd-backup-dir/system:etcd-peer-NODE_NAME.key
      resources:
        requests:
          memory: 60Mi
          cpu: 30m
      securityContext:
        privileged: true
      volumeMounts:
        - mountPath: /etc/kubernetes/etcd-backup-dir
          name: etcd-backup-dir
        - mountPath: /etc/kubernetes/static-pod-resources
          name: resource-dir
        - mountPath: /etc/kubernetes/static-pod-certs
          name: cert-dir
  containers:
  # The etcdctl container should always be first. It is intended to be used
  # to open a remote shell via ` + "`" + `oc rsh` + "`" + ` that is ready to run ` + "`" + `etcdctl` + "`" + `.
  - name: etcdctl
    image: ${IMAGE}
    imagePullPolicy: IfNotPresent
    terminationMessagePolicy: FallbackToLogsOnError
    command:
      - "/bin/bash"
      - "-c"
      - "trap: TERM INT; sleep infinity & wait"
    resources:
      requests:
        memory: 60Mi
        cpu: 30m
    volumeMounts:
      - mountPath: /etc/kubernetes/manifests
        name: static-pod-dir
      - mountPath: /etc/kubernetes/etcd-backup-dir
        name: etcd-backup-dir
      - mountPath: /etc/kubernetes/static-pod-resources
        name: resource-dir
      - mountPath: /etc/kubernetes/static-pod-certs
        name: cert-dir
      - mountPath: /var/lib/etcd/
        name: data-dir
    env:
${COMPUTED_ENV_VARS}
  - name: etcd
    image: ${IMAGE}
    imagePullPolicy: IfNotPresent
    terminationMessagePolicy: FallbackToLogsOnError
    command:
      - /bin/sh
      - -c
      - |
        #!/bin/sh
        set -euo pipefail

        etcdctl member list || true

        # this has a non-zero return code if the command is non-zero.  If you use an export first, it doesn't and you
        # will succeed when you should fail.
        ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster \
          --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \
          --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \
          --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \
          --endpoints=${ALL_ETCD_ENDPOINTS} \
          --data-dir=/var/lib/etcd/member \
          --target-peer-url-host=${NODE_NODE_ENVVAR_NAME_ETCD_DNS_NAME} \
          --target-name=NODE_NAME)
         export ETCD_INITIAL_CLUSTER

        # at this point we know this member is added.  To support a transition, we must remove the old etcd pod.
        # move it somewhere safe so we can retrieve it again later if something goes badly.
        mv /etc/kubernetes/manifests/etcd-member.yaml /etc/kubernetes/etcd-backup-dir || true

        export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME}
        env | grep ETCD | grep -v NODE

        set -x
        exec etcd \
          --initial-advertise-peer-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2380 \
          --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.crt \
          --key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.key \
          --trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \
          --client-cert-auth=true \
          --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \
          --peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \
          --peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \
          --peer-client-cert-auth=true \
          --advertise-client-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2379 \
          --listen-client-urls=https://${LISTEN_ON_ALL_IPS}:2379 \
          --listen-peer-urls=https://${LISTEN_ON_ALL_IPS}:2380 \
          --listen-metrics-urls=https://${LISTEN_ON_ALL_IPS}:9978 ||  mv /etc/kubernetes/etcd-backup-dir/etcd-member.yaml /etc/kubernetes/manifests
    env:
${COMPUTED_ENV_VARS}
    resources:
      requests:
        memory: 600Mi
        cpu: 300m
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -ec
          - "lsof -n -i :2380 | grep LISTEN"
      failureThreshold: 3
      initialDelaySeconds: 3
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 5
    securityContext:
      privileged: true
    volumeMounts:
      - mountPath: /etc/kubernetes/manifests
        name: static-pod-dir
      - mountPath: /etc/kubernetes/etcd-backup-dir
        name: etcd-backup-dir
      - mountPath: /etc/kubernetes/static-pod-resources
        name: resource-dir
      - mountPath: /etc/kubernetes/static-pod-certs
        name: cert-dir
      - mountPath: /var/lib/etcd/
        name: data-dir
  - name: etcd-metrics
    image: ${IMAGE}
    imagePullPolicy: IfNotPresent
    terminationMessagePolicy: FallbackToLogsOnError
    command:
      - /bin/sh
      - -c
      - |
        #!/bin/sh
        set -euo pipefail

        export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME}

        exec etcd grpc-proxy start \
          --endpoints https://${NODE_NODE_ENVVAR_NAME_ETCD_DNS_NAME}:9978 \
          --metrics-addr https://${LISTEN_ON_ALL_IPS}:9979 \
          --listen-addr ${LOCALHOST_IP}:9977 \
          --key /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \
          --key-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.key \
          --cert /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \
          --cert-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.crt \
          --cacert /etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \
          --trusted-ca-file /etc/kubernetes/static-pod-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crt
    env:
${COMPUTED_ENV_VARS}
    resources:
      requests:
        memory: 200Mi
        cpu: 100m
    securityContext:
      privileged: true
    volumeMounts:
      - mountPath: /etc/kubernetes/static-pod-resources
        name: resource-dir
      - mountPath: /etc/kubernetes/static-pod-certs
        name: cert-dir
      - mountPath: /var/lib/etcd/
        name: data-dir
  hostNetwork: true
  priorityClassName: system-node-critical
  tolerations:
  - operator: "Exists"
  volumes:
    - hostPath:
        path: /etc/kubernetes/manifests
      name: static-pod-dir
    - hostPath:
        path: /etc/kubernetes/static-pod-resources/etcd-member
      name: etcd-backup-dir
    - hostPath:
        path: /etc/kubernetes/static-pod-resources/etcd-pod-REVISION
      name: resource-dir
    - hostPath:
        path: /etc/kubernetes/static-pod-resources/etcd-certs
      name: cert-dir
    - hostPath:
        path: /var/lib/etcd
        type: ""
      name: data-dir

`)

func etcdPodYamlBytes() ([]byte, error) {
	return _etcdPodYaml, nil
}

func etcdPodYaml() (*asset, error) {
	bytes, err := etcdPodYamlBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/pod.yaml", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdRestorePodCmYaml = []byte(`apiVersion: v1
kind: ConfigMap
metadata:
  namespace: openshift-etcd
  name: restore-etcd-pod
data:
  pod.yaml:
`)

func etcdRestorePodCmYamlBytes() ([]byte, error) {
	return _etcdRestorePodCmYaml, nil
}

func etcdRestorePodCmYaml() (*asset, error) {
	bytes, err := etcdRestorePodCmYamlBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/restore-pod-cm.yaml", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdRestorePodYaml = []byte(`apiVersion: v1
kind: Pod
metadata:
  name: etcd
  namespace: openshift-etcd
  labels:
    app: etcd
    k8s-app: etcd
    etcd: "true"
    revision: "REVISION"
spec:
  containers:
  - name: etcd
    image: ${IMAGE}
    imagePullPolicy: IfNotPresent
    terminationMessagePolicy: FallbackToLogsOnError
    command:
      - /bin/sh
      - -c
      - |
        #!/bin/sh
        set -euo pipefail

        export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME}
        export ETCD_INITIAL_CLUSTER="${ETCD_NAME}=https://${NODE_NODE_ENVVAR_NAME_ETCD_DNS_NAME}:2380"
        env | grep ETCD | grep -v NODE
        export ETCD_NODE_PEER_URL=https://${NODE_NODE_ENVVAR_NAME_ETCD_DNS_NAME}:2380

        # checking if data directory is empty, if not etcdctl restore will fail
        if [ ! -z $(ls -A "/var/lib/etcd") ]; then
          echo "please delete the contents of data directory before restoring, running the restore script will do this for you"
          exit 1
        fi

        # check if we have backup file to be restored
        # if the file exist, check if it has not changed size in last 5 seconds
        if [ ! -f /var/lib/etcd-backup/snapshot.db ]; then
          echo "please make a copy of the snapshot db file, then move that copy to /var/lib/etcd-backup/snapshot.db"
          exit 1
        else
          filesize=$(stat --format=%s "/var/lib/etcd-backup/snapshot.db")
          sleep 5
          newfilesize=$(stat --format=%s "/var/lib/etcd-backup/snapshot.db")
          if [ "$filesize" != "$newfilesize" ]; then
            echo "file size has changed since last 5 seconds, retry sometime after copying is complete"
            exit 1
          fi
        fi

        UUID=$(uuidgen)
        echo "restoring to a single node cluster"
        ETCDCTL_API=3 /usr/bin/etcdctl snapshot restore /var/lib/etcd-backup/snapshot.db \
         --name  $ETCD_NAME \
         --initial-cluster=$ETCD_INITIAL_CLUSTER \
         --initial-cluster-token "openshift-etcd-{$UUID:0:10}" \
         --initial-advertise-peer-urls $ETCD_NODE_PEER_URL \
         --data-dir="/var/lib/etcd/restore-{$UUID:0:10}"

        mv /var/lib/etcd/restore-{$UUID:0:10}/* /var/lib/etcd/

        set -x
        exec etcd \
          --initial-advertise-peer-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2380 \
          --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.crt \
          --key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.key \
          --trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \
          --client-cert-auth=true \
          --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \
          --peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \
          --peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \
          --peer-client-cert-auth=true \
          --advertise-client-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2379 \
          --listen-client-urls=https://${LISTEN_ON_ALL_IPS}:2379 \
          --listen-peer-urls=https://${LISTEN_ON_ALL_IPS}:2380 \
          --listen-metrics-urls=https://${LISTEN_ON_ALL_IPS}:9978 ||  mv /etc/kubernetes/etcd-backup-dir/etcd-member.yaml /etc/kubernetes/manifests
    env:
${COMPUTED_ENV_VARS}
    resources:
      requests:
        memory: 600Mi
        cpu: 300m
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -ec
          - "lsof -n -i :2380 | grep LISTEN"
      failureThreshold: 3
      initialDelaySeconds: 3
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 5
    securityContext:
      privileged: true
    volumeMounts:
      - mountPath: /etc/kubernetes/manifests
        name: static-pod-dir
      - mountPath: /etc/kubernetes/etcd-backup-dir
        name: etcd-backup-dir
      - mountPath: /etc/kubernetes/static-pod-certs
        name: cert-dir
      - mountPath: /var/lib/etcd/
        name: data-dir
      - mountPath: /var/lib/etcd-backup/
        name: backup-dir
  hostNetwork: true
  priorityClassName: system-node-critical
  tolerations:
  - operator: "Exists"
  volumes:
    - hostPath:
        path: /etc/kubernetes/manifests
      name: static-pod-dir
    - hostPath:
        path: /etc/kubernetes/static-pod-resources/etcd-member
      name: etcd-backup-dir
    - hostPath:
        path: /etc/kubernetes/static-pod-resources/etcd-certs
      name: cert-dir
    - hostPath:
        path: /var/lib/etcd
        type: ""
      name: data-dir
    - hostPath:
        path: /var/lib/etcd-backup
        type: ""
      name: backup-dir

`)

func etcdRestorePodYamlBytes() ([]byte, error) {
	return _etcdRestorePodYaml, nil
}

func etcdRestorePodYaml() (*asset, error) {
	bytes, err := etcdRestorePodYamlBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/restore-pod.yaml", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdSaYaml = []byte(`apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: openshift-etcd
  name: etcd-sa
`)

func etcdSaYamlBytes() ([]byte, error) {
	return _etcdSaYaml, nil
}

func etcdSaYaml() (*asset, error) {
	bytes, err := etcdSaYamlBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/sa.yaml", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdScriptsCmYaml = []byte(`apiVersion: v1
kind: ConfigMap
metadata:
  namespace: openshift-etcd
  name: etcd-scripts
data:
  etcd.env:
  etcd-restore-backup.sh:
  etcd-snapshot-backup.sh:
  etcd-member-remove.sh:
  openshift-recovery-tools:
`)

func etcdScriptsCmYamlBytes() ([]byte, error) {
	return _etcdScriptsCmYaml, nil
}

func etcdScriptsCmYaml() (*asset, error) {
	bytes, err := etcdScriptsCmYamlBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/scripts-cm.yaml", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

var _etcdSvcYaml = []byte(`apiVersion: v1
kind: Service
metadata:
  namespace: openshift-etcd
  name: etcd
  annotations:
    service.alpha.openshift.io/serving-cert-secret-name: serving-cert
    prometheus.io/scrape: "true"
    prometheus.io/scheme: https
spec:
  selector:
    etcd: "true"
  ports:
  - name: https
    port: 443
    targetPort: 10257
`)

func etcdSvcYamlBytes() ([]byte, error) {
	return _etcdSvcYaml, nil
}

func etcdSvcYaml() (*asset, error) {
	bytes, err := etcdSvcYamlBytes()
	if err != nil {
		return nil, err
	}

	info := bindataFileInfo{name: "etcd/svc.yaml", size: 0, mode: os.FileMode(0), modTime: time.Unix(0, 0)}
	a := &asset{bytes: bytes, info: info}
	return a, nil
}

// Asset loads and returns the asset for the given name.
// It returns an error if the asset could not be found or
// could not be loaded.
func Asset(name string) ([]byte, error) {
	cannonicalName := strings.Replace(name, "\\", "/", -1)
	if f, ok := _bindata[cannonicalName]; ok {
		a, err := f()
		if err != nil {
			return nil, fmt.Errorf("Asset %s can't read by error: %v", name, err)
		}
		return a.bytes, nil
	}
	return nil, fmt.Errorf("Asset %s not found", name)
}

// MustAsset is like Asset but panics when Asset would return an error.
// It simplifies safe initialization of global variables.
func MustAsset(name string) []byte {
	a, err := Asset(name)
	if err != nil {
		panic("asset: Asset(" + name + "): " + err.Error())
	}

	return a
}

// AssetInfo loads and returns the asset info for the given name.
// It returns an error if the asset could not be found or
// could not be loaded.
func AssetInfo(name string) (os.FileInfo, error) {
	cannonicalName := strings.Replace(name, "\\", "/", -1)
	if f, ok := _bindata[cannonicalName]; ok {
		a, err := f()
		if err != nil {
			return nil, fmt.Errorf("AssetInfo %s can't read by error: %v", name, err)
		}
		return a.info, nil
	}
	return nil, fmt.Errorf("AssetInfo %s not found", name)
}

// AssetNames returns the names of the assets.
func AssetNames() []string {
	names := make([]string, 0, len(_bindata))
	for name := range _bindata {
		names = append(names, name)
	}
	return names
}

// _bindata is a table, holding each asset generator, mapped to its name.
var _bindata = map[string]func() (*asset, error){
	"etcd/cm.yaml":                  etcdCmYaml,
	"etcd/defaultconfig.yaml":       etcdDefaultconfigYaml,
	"etcd/etcd-member-remove.sh":    etcdEtcdMemberRemoveSh,
	"etcd/etcd-restore-backup.sh":   etcdEtcdRestoreBackupSh,
	"etcd/etcd-snapshot-backup.sh":  etcdEtcdSnapshotBackupSh,
	"etcd/ns.yaml":                  etcdNsYaml,
	"etcd/openshift-recovery-tools": etcdOpenshiftRecoveryTools,
	"etcd/pod-cm.yaml":              etcdPodCmYaml,
	"etcd/pod.yaml":                 etcdPodYaml,
	"etcd/restore-pod-cm.yaml":      etcdRestorePodCmYaml,
	"etcd/restore-pod.yaml":         etcdRestorePodYaml,
	"etcd/sa.yaml":                  etcdSaYaml,
	"etcd/scripts-cm.yaml":          etcdScriptsCmYaml,
	"etcd/svc.yaml":                 etcdSvcYaml,
}

// AssetDir returns the file names below a certain
// directory embedded in the file by go-bindata.
// For example if you run go-bindata on data/... and data contains the
// following hierarchy:
//     data/
//       foo.txt
//       img/
//         a.png
//         b.png
// then AssetDir("data") would return []string{"foo.txt", "img"}
// AssetDir("data/img") would return []string{"a.png", "b.png"}
// AssetDir("foo.txt") and AssetDir("notexist") would return an error
// AssetDir("") will return []string{"data"}.
func AssetDir(name string) ([]string, error) {
	node := _bintree
	if len(name) != 0 {
		cannonicalName := strings.Replace(name, "\\", "/", -1)
		pathList := strings.Split(cannonicalName, "/")
		for _, p := range pathList {
			node = node.Children[p]
			if node == nil {
				return nil, fmt.Errorf("Asset %s not found", name)
			}
		}
	}
	if node.Func != nil {
		return nil, fmt.Errorf("Asset %s not found", name)
	}
	rv := make([]string, 0, len(node.Children))
	for childName := range node.Children {
		rv = append(rv, childName)
	}
	return rv, nil
}

type bintree struct {
	Func     func() (*asset, error)
	Children map[string]*bintree
}

var _bintree = &bintree{nil, map[string]*bintree{
	"etcd": {nil, map[string]*bintree{
		"cm.yaml":                  {etcdCmYaml, map[string]*bintree{}},
		"defaultconfig.yaml":       {etcdDefaultconfigYaml, map[string]*bintree{}},
		"etcd-member-remove.sh":    {etcdEtcdMemberRemoveSh, map[string]*bintree{}},
		"etcd-restore-backup.sh":   {etcdEtcdRestoreBackupSh, map[string]*bintree{}},
		"etcd-snapshot-backup.sh":  {etcdEtcdSnapshotBackupSh, map[string]*bintree{}},
		"ns.yaml":                  {etcdNsYaml, map[string]*bintree{}},
		"openshift-recovery-tools": {etcdOpenshiftRecoveryTools, map[string]*bintree{}},
		"pod-cm.yaml":              {etcdPodCmYaml, map[string]*bintree{}},
		"pod.yaml":                 {etcdPodYaml, map[string]*bintree{}},
		"restore-pod-cm.yaml":      {etcdRestorePodCmYaml, map[string]*bintree{}},
		"restore-pod.yaml":         {etcdRestorePodYaml, map[string]*bintree{}},
		"sa.yaml":                  {etcdSaYaml, map[string]*bintree{}},
		"scripts-cm.yaml":          {etcdScriptsCmYaml, map[string]*bintree{}},
		"svc.yaml":                 {etcdSvcYaml, map[string]*bintree{}},
	}},
}}

// RestoreAsset restores an asset under the given directory
func RestoreAsset(dir, name string) error {
	data, err := Asset(name)
	if err != nil {
		return err
	}
	info, err := AssetInfo(name)
	if err != nil {
		return err
	}
	err = os.MkdirAll(_filePath(dir, filepath.Dir(name)), os.FileMode(0755))
	if err != nil {
		return err
	}
	err = ioutil.WriteFile(_filePath(dir, name), data, info.Mode())
	if err != nil {
		return err
	}
	err = os.Chtimes(_filePath(dir, name), info.ModTime(), info.ModTime())
	if err != nil {
		return err
	}
	return nil
}

// RestoreAssets restores an asset under the given directory recursively
func RestoreAssets(dir, name string) error {
	children, err := AssetDir(name)
	// File
	if err != nil {
		return RestoreAsset(dir, name)
	}
	// Dir
	for _, child := range children {
		err = RestoreAssets(dir, filepath.Join(name, child))
		if err != nil {
			return err
		}
	}
	return nil
}

func _filePath(dir, name string) string {
	cannonicalName := strings.Replace(name, "\\", "/", -1)
	return filepath.Join(append([]string{dir}, strings.Split(cannonicalName, "/")...)...)
}
